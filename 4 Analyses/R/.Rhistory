no_stop_final$final_affordance[no_stop_final$final_affordance == "don't"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he's"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it's"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "buyingselling"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "boatship"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "drawingwriting"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "foodsdesserts"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "onoff"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "andor"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "presentgift"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "ac"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "skincaremakeup"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "picturespicture"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
#let's try udpipe one more time
ud_out = udpipe(no_stop_final$affordance_corrected, "english")
View(ud_out)
14327/2
#figure out how to add cue
no_stop_final$key = rep(1:nrow(no_stop_final))
no_stop_key = no_stop_final[ , c(4, 10)]
ud_out$key = substring(ud_out$doc_id, 4)
merged = merge(ud_out, no_stop_key, "key")
View(merged)
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$upos != "PUNCT")
merged = subset(merged,
merged$upos != "PART")
merged = subset(merged,
merged$upos != "NUM")
merged = subset(merged,
merged$upos != "PRON")
merged = subset(merged,
merged$upos != "SCONJ")
merged = subset(merged,
merged$upos != "ADV")
merged = subset(merged,
merged$upos != "ADP")
##remove cans, woulds, shoulds, coulds, etc.
merged = subset(merged,
merged$lemma !="can")
merged = subset(merged,
merged$lemma !="would")
merged = subset(merged,
merged$lemma !="could")
merged = subset(merged,
merged$lemma !="have")
merged = subset(merged,
merged$lemma !="should")
merged = subset(merged,
merged$lemma !="a")
merged = subset(merged,
merged$lemma !="of")
merged = subset(merged,
merged$lemma !="the")
merged = subset(merged,
merged$lemma !="can")
merged = subset(merged,
merged$lemma !="who")
merged = subset(merged,
merged$lemma !="what")
merged = subset(merged,
merged$lemma !="where")
merged = subset(merged,
merged$lemma !="'s")
merged = subset(merged,
merged$lemma !="be")
merged = subset(merged,
merged$lemma !="do")
merged = subset(merged,
merged$lemma !="get")
merged = subset(merged,
merged$lemma !="may")
merged = subset(merged,
merged$lemma !="maybe")
merged = subset(merged,
merged$lemma !="will")
#remove other weirdness
merged = subset(merged,
merged$lemma !="roor")
merged = subset(merged,
merged$lemma !="dino")
merged = subset(merged,
merged$lemma !="ok")
merged = subset(merged,
merged$lemma !="no")
merged = subset(merged,
merged$lemma !="okay")
merged = subset(merged,
merged$lemma !="yes")
merged = subset(merged,
merged$lemma !="any")
merged = subset(merged,
merged$lemma !="every")
merged = subset(merged,
merged$lemma !="all")
merged = subset(merged,
merged$lemma !="this")
merged = subset(merged,
merged$lemma !="some")
merged = subset(merged,
merged$lemma !="that")
merged = subset(merged,
merged$lemma !="those")
merged = subset(merged,
merged$lemma !="quite")
merged = subset(merged,
merged$lemma !="each")
merged = subset(merged,
merged$lemma !="oto")
merged = subset(merged,
merged$lemma !="other")
merged = subset(merged,
merged$lemma !="another")
merged = subset(merged,
merged$lemma !="both")
#keep it going
table(merged$upos)
merged = subset(merged,
merged$lemma !="sandwhic")
merged = subset(merged,
merged$lemma !="anti")
merged = subset(merged,
merged$lemma !="stuff")
merged = subset(merged,
merged$lemma !="s")
merged = subset(merged,
merged$lemma !="semi")
merged = subset(merged,
merged$lemma !="non")
merged = subset(merged,
merged$lemma !="and")
merged = subset(merged,
merged$lemma !="or")
merged = subset(merged,
merged$lemma !="but")
merged = subset(merged,
merged$lemma !="thing")
merged = subset(merged,
merged$lemma !="lol")
#fix some misspellings
merged$lemma[merged$lemma == "headbut"] = "headbutt"
##take a look in excel
##take a look in excel
#write.csv(merged, file = "udpipe_5_24.csv", row.names = F)
##take a look in excel
write.csv(merged, file = "udpipe_5_24.csv", row.names = F)
####Remove duplicated rows####
dat = read.csv("udpipe_5_24.csv")
dat2 = dat %>% distinct(sentence, lemma, .keep_all = T)
View(dat2)
View(dat)
##write to .csv for inspection
write.csv(dat2, file = "No_duplicates_5_24.csv", row.names = F)
setwd("~/GitHub/BOI-Norms/4 Analyses/R")
##Combine data into master file
files = list.files(pattern = "*.csv")
##Combine data into master file
setwd("./Cleaned Data Sets")
gegetwd()
getwd()
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
getwd()
#Now do the pre-cleaning (this is to pull subIDs)
setwd("./Raw Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat2 = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
####Descriptives####
##Number of subjects
length(unique(dat2$ID))
#total number or responses (All tenses)
temp2 = data.frame()
for(i in unique(dat$CUE)){
temp = subset(dat,
dat$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp2 = rbind(temp2, temp)
}
mean(temp2$RESPONSE_NUM) ##Across all POS, each affordance has an average of 14 responses
View(temp2)
View(dat)
####Set up####
##Combine data into master file
#Start w/ cleaned data
setwd("./Cleaned Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
#Now do the pre-cleaning (this is to pull subIDs)
setwd("./Raw Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat2 = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
####Descriptives####
##Number of subjects
length(unique(dat2$ID)) #796
#total number or responses (All tenses)
temp2 = data.frame()
for(i in unique(dat$CUE)){
temp = subset(dat,
dat$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp2 = rbind(temp2, temp)
}
mean(temp2$RESPONSE_NUM) ##Across all POS, each affordance has an average of 14 responses
##total number of VERB responses
#make a verb subset
VERB = subset(dat,
dat$POS == "VERB")
temp4 = data.frame()
for(i in unique(VERB$CUE)){
temp = subset(VERB,
VERB$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp4 = rbind(temp4, temp)
}
mean(temp4$RESPONSE_NUM) ##On average, 6 verb responses
###total number of NOUN responses
NOUN = subset(dat,
dat$POS == "NOUN")
temp6 = data.frame()
for(i in unique(NOUN$CUE)){
temp = subset(NOUN,
NOUN$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp6 = rbind(temp6, temp)
}
mean(temp6$RESPONSE_NUM) #average of 8 NOUN responses
##write to file
write.csv(dat, file = "All_responses.csv", row.names = F)
setwd("~/GitHub/BOI-Norms/4 Analyses/R/Batch 1 October 20 - March 22")
##read in data
master = read.csv("0-Data/combined_data_3_16_22.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33, 35)]
View(dat)
View(master)
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("remove idk.R")
setwd("~/GitHub/BOI-Norms/4 Analyses/R/Batch 1 October 20 - March 22/Scripts")
source("remove idk.R")
#Check for NAs
table(is.na(dat$affordance_response))
#remove nas
dat = na.omit(dat)
parsed_afforances = unnest_tokens(tbl = dat, output = parsed,
input = affordance_response, token = "regex",
pattern = ", ")
View(parsed_afforances)
wordlist = unique(parsed_afforances$parsed)
#Run the spell check
spelling.errors = hunspell(wordlist)
spelling.errors = unique(unlist(spelling.errors))
spelling.sugg = hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
#Pick the first spelling suggestion
spelling.sugg = unlist(lapply(spelling.sugg, function(x) x[1]))
#manually check errors
spell_check = cbind(spelling.sugg, spelling.errors)
#read back in the checked output
spell_check = read.csv("spell_check.csv", stringsAsFactors = F)
setwd("~/GitHub/BOI-Norms/4 Analyses/R/Batch 1 October 20 - March 22")
#read back in the checked output
spell_check = read.csv("spell_check.csv", stringsAsFactors = F)
#get the correct number of observations and make a spelling dictionary
spelling.errors = as.data.frame(spelling.errors)
spelling.dict = (merge(spelling.errors, spell_check, by = 'spelling.errors'))
spelling.dict$spelling.sugg = tolower(spelling.dict$spelling.sugg)
spelling.dict$spelling.pattern = paste0("\\b", spelling.dict$spelling.errors, "\\b")
##Remove white spaces and replace misspellings
parsed_afforances = parsed_afforances[!parsed_afforances$parsed =="", ]
parsed_afforances = unnest_tokens(tbl = parsed_afforances, output = parsed,
input = parsed, token = "regex",
pattern = ",")
parsed_afforances$corrected = stri_replace_all_regex(str = parsed_afforances$parsed,
pattern = spelling.dict$spelling.pattern,
replacement = spelling.dict$spelling.sugg,
vectorize_all = FALSE)
##Remove a few weird things
parsed_afforances$corrected[parsed_afforances$corrected == "na"] = NA
parsed_afforances$corrected[parsed_afforances$corrected == " "] = NA
parsed_afforances$corrected[parsed_afforances$corrected == ""] = NA
parsed_afforances$corrected[parsed_afforances$corrected == "  learning how to do a task"] = "someone learning how to do a task"
#Fix column names
colnames(parsed_afforances)[7:8] = c("affordance_parsed", "affordance_corrected")
##Write spelled checked data to .csv
write.csv(parsed_afforances, file = "spell_checked.csv", row.names = F)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
View(dat)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
colnames(dat)[8] = "Inital"
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
colnames(dat)[8] = "Initial"
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
##compound words
no_stop$cleaned[no_stop$cleaned == "ice cream"] = "icecream"
no_stop$cleaned[no_stop$cleaned == "make up"] = "makeup"
no_stop$cleaned[no_stop$cleaned == "thrown away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throw away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throwing away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "hi school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high five"] = "highfive"
no_stop$cleaned[no_stop$cleaned == "hi five"] = "highfive"
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
View(tokens)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don't"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he's"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it's"] = NA
no_stop_final$parsed[no_stop_final$parsed == "n/a"] = NA
no_stop_final$parsed[no_stop_final$parsed == "hail/call"] = NA
no_stop_final$parsed[no_stop_final$parsed == "fish/insect food"] = NA
no_stop_final$parsed[no_stop_final$parsed == "cover/smother"] = NA
View(no_stop_final)
setwd("~/GitHub/BOI-Norms/4 Analyses/R")
####Set up####
##Combine data into master file
#Start w/ cleaned data
setwd("./Cleaned Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
#Now do the pre-cleaning (this is to pull subIDs)
setwd("./Raw Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat2 = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
View(dat)
####Descriptives####
##Number of subjects
length(unique(dat2$ID)) #796
sd(temp6$RESPONSE_NUM)
####Set up####
##Combine data into master file
#Start w/ cleaned data
setwd("./Cleaned Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
#Now do the pre-cleaning (this is to pull subIDs)
setwd("./Raw Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat2 = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
####Descriptives####
##Number of subjects
length(unique(dat2$ID)) #796
#total number or responses (All tenses)
temp2 = data.frame()
for(i in unique(dat$CUE)){
temp = subset(dat,
dat$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp2 = rbind(temp2, temp)
}
mean(temp2$RESPONSE_NUM) ##Across all POS, each affordance has an average of 15 responses
##total number of VERB responses
#make a verb subset
VERB = subset(dat,
dat$POS == "VERB")
temp4 = data.frame()
for(i in unique(VERB$CUE)){
temp = subset(VERB,
VERB$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp4 = rbind(temp4, temp)
}
mean(temp4$RESPONSE_NUM) ##On average, 7 verb responses
###total number of NOUN responses
NOUN = subset(dat,
dat$POS == "NOUN")
temp6 = data.frame()
for(i in unique(NOUN$CUE)){
temp = subset(NOUN,
NOUN$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp6 = rbind(temp6, temp)
}
mean(temp6$RESPONSE_NUM) #average of 8 NOUN responses
####Set up####
##Combine data into master file
#Start w/ cleaned data
setwd("./Cleaned Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
#Now do the pre-cleaning (this is to pull subIDs)
setwd("./Raw Data Sets")
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat2 = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
####Descriptives####
##Number of subjects
length(unique(dat2$ID)) #796
##How many times has each word been normed in the final data set?
#total number or responses (All tenses)
temp2 = data.frame()
for(i in unique(dat$CUE)){
temp = subset(dat,
dat$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp2 = rbind(temp2, temp)
}
mean(temp2$RESPONSE_NUM) ##Across all POS, each affordance has an average of 15 responses
##total number of VERB responses
#make a verb subset
VERB = subset(dat,
dat$POS == "VERB")
temp4 = data.frame()
for(i in unique(VERB$CUE)){
temp = subset(VERB,
VERB$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp4 = rbind(temp4, temp)
}
mean(temp4$RESPONSE_NUM) ##On average, 7 verb responses
###total number of NOUN responses
NOUN = subset(dat,
dat$POS == "NOUN")
temp6 = data.frame()
for(i in unique(NOUN$CUE)){
temp = subset(NOUN,
NOUN$CUE == i)
nrow(temp)
temp$RESPONSE_NUM = rep(nrow(temp))
temp6 = rbind(temp6, temp)
}
mean(temp6$RESPONSE_NUM) #average of 8 NOUN responses
####Plan####
##Master sheet -- (Nouns and Verbs)
##Verb Tab, Noun Tab
##write to file
#write.csv(dat, file = "All_responses.csv", row.names = F)
##write to file
write.csv(dat, file = "All_responses.csv", row.names = F)
