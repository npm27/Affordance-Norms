##Two-year
out = table(seventeen_eighteen_t$ID %in% nineteen_twenty_t$ID) #62
##Two-year
out = table(seventeen_eighteen_t$ID %in% nineteen_twenty_t$ID) #62
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
#One-year
out = table(eighteen_nineteen_t$ID %in% nineteen_twenty_t$ID) #79
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
#One-year
out = table(eighteen_nineteen_t$ID %in% nineteen_twenty_t$ID) #79
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##Year-to-year retention
##15/16 - 16/17
out = table(fifteen_sixteen_t$ID %in% sixteen_seventeen_t$ID) #78
(out[2]/length(unique(sixteen_seventeen_t$ID))) * 100
##16/17 - 17/18
out = table(sixteen_seventeen_t$ID %in% seventeen_eighteen_t$ID) #81
(out[2]/length(unique(seventeen_eighteen_t$ID))) * 100
#18/19 - 19/20
out = table(eighteen_nineteen_t$ID %in% nineteen_twenty_t$ID) #79
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
#One-year
out = table(eighteen_nineteen_tt$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 #about 99%
##Two-year
out = table(seventeen_eighteen_tt$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 #about 84%
##Three-year
out = table(sixteen_seventeen$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 #about 86%
##Overall Retention
##four-year
out = table(fifteen_sixteen_tt$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 #about 74%
##Two-year
out = table(seventeen_eighteen_tt$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 #about 84%
##Year-to-year retention
##15/16 - 16/17
out = table(fifteen_sixteen_tt$ID %in% sixteen_seventeen_tt$ID)
(out[2]/length(unique(sixteen_seventeen_tt$ID))) * 100 #about 90%
##16/17 - 17/18
out = table(sixteen_seventeen_tt$ID %in% seventeen_eighteen_tt$ID)
(out[2]/length(unique(seventeen_eighteen_tt$ID))) * 100 #about 96%
##16/17 - 17/18
out = table(sixteen_seventeen_tt$ID %in% seventeen_eighteen_tt$ID)
(out[2]/length(unique(seventeen_eighteen_tt$ID))) * 100 #about 96%
##17/18 - 18/19
out = table(seventeen_eighteen_tt$ID %in% eighteen_nineteen_tt$ID)
(out[2]/length(unique(eighteen_nineteen_tt$ID))) * 100 ##about 90%
#18/19 - 19/20
out = table(eighteen_nineteen_tt$ID %in% nineteen_twenty_tt$ID)
(out[2]/length(unique(nineteen_twenty_tt$ID))) * 100 ##about 90%
##Overall Retention
##four-year
out = table(fifteen_sixteen_nt$ID %in% nineteen_twenty_nt$ID)
(out[2]/length(unique(nineteen_twenty_nt$ID))) * 100 #about 72%
##Three-year
out = table(sixteen_seventeen$ID %in% nineteen_twenty_nt$ID)
(out[2]/length(unique(nineteen_twenty_nt$ID))) * 100 #about 82%
#One-year
out = table(eighteen_nineteen_nt$ID %in% nineteen_twenty_nt$ID)
(out[2]/length(unique(nineteen_twenty_nt$ID))) * 100 #about 84%
##Two-year
out = table(seventeen_eighteen_nt$ID %in% nineteen_twenty_nt$ID)
(out[2]/length(unique(nineteen_twenty_nt$ID))) * 100 #about 82%
##Year-to-year retention
##15/16 - 16/17
out = table(fifteen_sixteen_nt$ID %in% sixteen_seventeen_nt$ID)
(out[2]/length(unique(sixteen_seventeen_nt$ID))) * 100 #about 72%
##Technical/paraprofessional
fifteen_sixteen_t = subset(fifteen_sixteen_ng,
fifteen_sixteen_ng$EECODE == 4)
sixteen_seventeen_t = subset(sixteen_seventeen_ng,
sixteen_seventeen_ng$EECODE == 4)
seventeen_eighteen_t = subset(seventeen_eighteen_ng,
seventeen_eighteen_ng$EECODE == 4)
eighteen_nineteen_t = subset(eighteen_nineteen_ng,
eighteen_nineteen_ng$EECODE == 4)
nineteen_twenty_t = subset(nineteen_twenty_ng,
nineteen_twenty_ng$EECODE == 4)
##Overall Retention
##four-year
out = table(fifteen_sixteen_t$ID %in% nineteen_twenty_t$ID)
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100 #43
##Three-year
out = table(sixteen_seventeen$ID %in% nineteen_twenty_t$ID)
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100 #62
##Two-year
out = table(seventeen_eighteen_t$ID %in% nineteen_twenty_t$ID)
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100 #56
#One-year
out = table(eighteen_nineteen_t$ID %in% nineteen_twenty_t$ID)
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100 #78
##clerical/secretarial
fifteen_sixteen_t = subset(fifteen_sixteen_ng,
fifteen_sixteen_ng$EECODE == 5)
sixteen_seventeen_t = subset(sixteen_seventeen_ng,
sixteen_seventeen_ng$EECODE == 5)
seventeen_eighteen_t = subset(seventeen_eighteen_ng,
seventeen_eighteen_ng$EECODE == 5)
eighteen_nineteen_t = subset(eighteen_nineteen_ng,
eighteen_nineteen_ng$EECODE == 5)
nineteen_twenty_t = subset(nineteen_twenty_ng,
nineteen_twenty_ng$EECODE == 5)
##Overall Retention
##four-year
out = table(fifteen_sixteen_t$ID %in% nineteen_twenty_t$ID) #36
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##Three-year
out = table(sixteen_seventeen$ID %in% nineteen_twenty_t$ID) #54
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##Two-year
out = table(seventeen_eighteen_t$ID %in% nineteen_twenty_t$ID) #54
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
#One-year
out = table(eighteen_nineteen_t$ID %in% nineteen_twenty_t$ID) #72
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##Year-to-year retention
##15/16 - 16/17
out = table(fifteen_sixteen_t$ID %in% sixteen_seventeen_t$ID) ##77
(out[2]/length(unique(sixteen_seventeen_t$ID))) * 100
##skilled crafts
fifteen_sixteen_t = subset(fifteen_sixteen_ng,
fifteen_sixteen_ng$EECODE == 6)
sixteen_seventeen_t = subset(sixteen_seventeen_ng,
sixteen_seventeen_ng$EECODE == 6)
seventeen_eighteen_t = subset(seventeen_eighteen_ng,
seventeen_eighteen_ng$EECODE == 6)
eighteen_nineteen_t = subset(eighteen_nineteen_ng,
eighteen_nineteen_ng$EECODE == 6)
nineteen_twenty_t = subset(nineteen_twenty_ng,
nineteen_twenty_ng$EECODE == 6)
##Overall Retention
##four-year
out = table(fifteen_sixteen_t$ID %in% nineteen_twenty_t$ID) #43
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##service/maintenance
fifteen_sixteen_t = subset(fifteen_sixteen_ng,
fifteen_sixteen_ng$EECODE == 7)
sixteen_seventeen_t = subset(sixteen_seventeen_ng,
sixteen_seventeen_ng$EECODE == 7)
seventeen_eighteen_t = subset(seventeen_eighteen_ng,
seventeen_eighteen_ng$EECODE == 7)
eighteen_nineteen_t = subset(eighteen_nineteen_ng,
eighteen_nineteen_ng$EECODE == 7)
nineteen_twenty_t = subset(nineteen_twenty_ng,
nineteen_twenty_ng$EECODE == 7)
##Overall Retention
##four-year
out = table(fifteen_sixteen_t$ID %in% nineteen_twenty_t$ID) #45
(out[2]/length(unique(nineteen_twenty_t$ID))) * 100
##Year-to-year retention
##15/16 - 16/17
out = table(fifteen_sixteen_t$ID %in% sixteen_seventeen_t$ID) #78
(out[2]/length(unique(sixteen_seventeen_t$ID))) * 100
View(eighteen_nineteen)
setwd("~/")
library(readxl)
JAM = read_xlsx("E4.xlsx", sheet = "JAM")
JOL = read_xlsx("E4.xlsx", sheet = "JOL")
Read = read_xlsx("E4.xlsx", sheet = "read")
JAM$task = rep("JAM")
JOL$task = rep("JOL")
Read$task = rep("read")
combined = rbind(JAM, JOL, Read)
library(reshape)
library(ez)
View(combined)
long = melt(combined, measure.vars = "Backward", "Forward", "Symmetrical", "Unrelated")
long = melt(combined, measure.vars = "Backward", "Forward", "Symmetrical", "Unrelated")
long = melt(combined, measure.vars = c("Backward", "Forward", "Symmetrical", "Unrelated"))
long = melt(combined, measure.vars = c("Backward", "Forward", "Symmetrical", "Unrelated"))
long = melt(combined, id.vars = c(SUB.ID, task))
long = melt(combined, id.vars = c("SUB.ID", "task"))
long = melt(combined)
melt(combined)
library(data.table)
melt(combined, measure.vars = c("Backward", "Forward"))
melt(combined, measure.vars = c("Backward", "Forward", "Symmetrical", "Unrelated"))
long = melt(combined, measure.vars = c("Backward", "Forward", "Symmetrical", "Unrelated"))
View(long)
colnames(long)[3:4] = c("Direction", "Scored")
ezANOVA(long,
wid = SUB.ID,
between = task,
within = Direction,
dv = Scored)
installed.packages("psychReport")
install.packages("psychReport")
library(psychReport)
model1 = ezANOVA(long,
wid = SUB.ID,
between = task,
within = Direction,
dv = Scored)
aovEffectSize(model1, effectSize = "pes")
model1 = ezANOVA(long,
wid = SUB.ID,
between = task,
within = Direction,
dv = Scored,
return_aov = TRUE)
aovEffectSize(model1, effectSize = "pes")
model1 = ezANOVA(long,
wid = SUB.ID,
between = task,
within = Direction,
dv = Scored,
return_aov = TRUE,
detailed = TRUE)
aovEffectSize(model1, effectSize = "pes")
##Okay, what's up w/ these t-tests?
t.test(JAM$Forward, JOL$Forward, paired = F)
##Okay, what's up w/ these t-tests?
t.test(JAM$Forward, JOL$Forward, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JAM$Forward, Read$Forward, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JOL$Forward, Read$Forward, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JOL$Backward, Read$Backward, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JAM$Backward, Read$Backward, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JAM$Symmetrical, Read$Symmetrical, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JOL$Symmetrical, Read$Symmetrical, paired = F, var.equal = T)
##Okay, what's up w/ these t-tests?
t.test(JOL$Unrelated, Read$Unrelated, paired = F, var.equal = T)
mean(JAM$Unrelated)
##Okay, what's up w/ these t-tests?
t.test(JAM$Unrelated, Read$Unrelated, paired = F, var.equal = T)
#load libraries
library(caret)
library(data.table)
library(kernlab)
library(caTools)
##read in the data
test = read.csv("2 Cleaned Data/cohort 4181 cleaned.csv")
training = read.csv("2 Cleaned Data/Full Cohort cleaned.csv")
##Set Training Parameters
trainingParameters = trainControl(method = "repeatedcv", number = 3, repeats = 5, classProbs = T)
trainingParameters2 = trainControl(method = "repeatedcv", number = 10, repeats = 10, classProbs = T)
##Probably going to need to factor categorical stuff
##Make a forrest county flag?
table(training$County)
training$County == "FORREST"
training$County_Flag = training$County == "FORREST"
##Make an ethnicity flag?
#could either do it as white/minority or White (0), black (1), Hispanic (2), Asian (3), other (4)
table(training$EthnicGrp_A) #1 pacif, 12 native american, 463 non-specified, 44 multi -- thinking about combining on this
training$ETHNICITY = training$EthnicGrp_A
training$ETHNICITY[training$ETHNICITY == "PACIF"] = "OTHER"
training$ETHNICITY[training$ETHNICITY == "AMIND"] = "OTHER"
training$ETHNICITY[training$ETHNICITY == "MULTI"] = "OTHER"
training$ETHNICITY[training$ETHNICITY == "NSPEC"] = "OTHER"
table(training$ETHNICITY)
training$ETHNICITY = factor(training$ETHNICITY)
##Country - International Flag?
table(training$Country)
training$International_Flag = training$Country != "USA"
##Need to figure out department code flag
#Okay, thinking I might cut department and just do it based on college?
table(training$PriCollCd) #Arts and letters can be reference group (pretty sure R sets that alphabetically)
training$College = as.factor(training$PriCollCd)
##Make a hattiesburg campus flag
table(training$AppAdmCampus)
training$campus = training$AppAdmCampus == "HBG"
training$campus = as.numeric(training$campus)
training$campus = (training$campus - 1) * -1
table(training$campus)
training$campus = factor(training$campus,
levels = c(0, 1),
labels = c("HBG", "other"))
##Now add the flags to the test data
##Make a forrest county flag?
table(test$County)
test$County == "FORREST"
test$County_Flag = test$County == "FORREST"
##Ethnicity
table(test$EthnicGrp_A) #1 pacif, 12 native american, 463 non-specified, 44 multi -- thinking about combining on this
test$ETHNICITY = test$EthnicGrp_A
test$ETHNICITY[test$ETHNICITY == "PACIF"] = "OTHER"
test$ETHNICITY[test$ETHNICITY == "AMIND"] = "OTHER"
test$ETHNICITY[test$ETHNICITY == "MULTI"] = "OTHER"
test$ETHNICITY[test$ETHNICITY == "NSPEC"] = "OTHER"
table(test$ETHNICITY)
test$ETHNICITY = factor(test$ETHNICITY)
##Country - International Flag
table(test$Country)
test$International_Flag = test$Country != "USA"
##college codes
table(test$PriCollCd) #Arts and letters can be reference group (pretty sure R sets that alphabetically)
test$College = as.factor(test$PriCollCd)
##Make a hattiesburg campus flag
table(test$AppAdmCampus)
test$campus = test$AppAdmCampus == "HBG"
test$campus = as.numeric(test$campus)
test$campus = (test$campus - 1) * -1
table(test$campus)
test$campus = factor(test$campus,
levels = c(0, 1),
labels = c("HBG", "other"))
####Fix Athlete, PELL, Honors, and Greek flags####
##Athlete
unique(test$Athlete_Flag)
unique(training$Athlete_Flag)
test$Athlete_Flag[test$Athlete_Flag == "No_Athlete"] = "N"
test$Athlete_Flag[test$Athlete_Flag == "Yes_Athlete"] = "Y"
training$Athlete_Flag[training$Athlete_Flag == "No_Athlete"] = "N"
training$Athlete_Flag[training$Athlete_Flag == "Yes_Athlete"] = "Y"
##Pell
unique(test$Pell_Flag)
unique(training$Pell_Flag)
test$Pell_Flag[test$Pell_Flag == "No_Pell"] = "N"
test$Pell_Flag[test$Pell_Flag == "Yes_Pell"] = "Y"
training$Pell_Flag[training$Pell_Flag == "No_Pell"] = "N"
training$Pell_Flag[training$Pell_Flag == "Yes_Pell"] = "Y"
##honors
unique(test$honors_flag)
unique(training$honors_flag)
test$honors_flag[test$honors_flag == "No_honors"] = "N"
test$honors_flag[test$honors_flag == "Yes_honors"] = "Y"
training$honors_flag[training$honors_flag == "No_honors"] = "N"
training$honors_flag[training$honors_flag == "Yes_honors"] = "Y"
##greek
unique(test$greek_flag)
unique(training$greek_flag)
test$greek_flag[test$greek_flag == "No_greek"] = "N"
test$greek_flag[test$greek_flag == "Yes_greek"] = "Y"
training$greek_flag[training$greek_flag == "No_greek"] = "N"
training$greek_flag[training$greek_flag == "Yes_greek"] = "Y"
##Fix college code
#Start w/ training
unique(training$College)
##Current codes
#"Business & Economic Development" "Nursing & Health Professions"    "Arts & Sciences"
#"Education & Human Sciences"
training$College = as.character(training$College)
training$College[training$College == "SCIENCE"] = "Arts & Sciences"
training$College[training$College == "ARTS & LET"] = "Arts & Sciences"
training$College[training$College == "HEALTH"] = "Nursing & Health Professions"
training$College[training$College == "BUSECODEV"] = "Business & Economic Development"
training$College[training$College == "ED PSY"] = "Education & Human Sciences"
training = subset(training,
training$College != "UGSTUDIES")
##okay, now factor the training data
training$College = factor(training$College)
##Now the test data
unique(test$College)
test$College = as.character(test$College)
test$College[test$College == "SCIENCE"] = "Arts & Sciences"
test$College[test$College == "ARTS & LET"] = "Arts & Sciences"
test$College[test$College == "HEALTH"] = "Nursing & Health Professions"
test$College[test$College == "BUSECODEV"] = "Business & Economic Development"
test$College[test$College == "ED PSY"] = "Education & Human Sciences"
test = subset(test,
test$College != "UGSTUDIES")
##okay, now factor the test data
test$College = factor(test$College)
setwd("~/GitHub/BOI-Norms/3 Output")
####Combine .csv files into master datasheet####
setwd("./RAW")
#Put them in one dataframe. First apply read.csv, then rbind
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
#Get the files names
files = list.files(pattern = "*.csv")
#Put them in one dataframe. First apply read.csv, then rbind
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
View(dat)
##remove instruction trials
dat2 = subset(dat,
dat$Procedure.Trial.Type != "Instruct")
##remove instruction trials
unique(dat$Procedure.Trial.Type)
setwd("..")
View(dat2)
dat2 = subset(dat,
dat$Procedure.Item < 3001)
####Combine .csv files into master datasheet####
setwd("./RAW")
#Get the files names
files = list.files(pattern = "*.csv")
#read everything in and merge into one dataframe.
dat = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
setwd("..")
##remove instruction trials
unique(dat$Procedure.Trial.Type)
dat2 = subset(dat,
dat$Procedure.Trial.Type != "Instruct")
##Remove practice trials
dat2 = subset(dat2,
dat2$Procedure.Item < 3001)
6778/length(unique(dat$ID))
length(unique(dat$ID))
17000/12
1416-590
##Write to .csv
write.csv(dat2, file = "merged_12_8.csv", row.names = F)
setwd("~/GitHub/BOI-Norms/4 Analyses")
##read in data
master = read.csv("merged_12_8.csv")
install.packages("here")
install.packages("hunspeel")
install.packages("hunspell")
install.packages("tidytext")
install.packages("stringi")
install.packages("stringi")
install.packages("koRpus")
install.packages("koRpus.lang.en")
install.packages("stopwords")
####Set up####
##libraries
library(dplyr)
library(here)
library(hunspell)
library(tidytext)
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
#stopwords
library(stopwords)
View(master)
##drop unused column
dat = master[ , -c(2:4, 6:7, 9:11, 14:17, 19, 22:24, 26:27, 30:32,34)]
View(dat)
#useful column names
colnames(dat)[12] = "affordance_response"
####Clean the data####
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, ouput = token, input = affordance_response)
View(master)
####Set up####
##libraries
library(dplyr)
library(here)
#Spelling
library(hunspell)
library(tidytext)
library()
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
#stopwords
library(stopwords)
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, ouput = token, input = affordance_response)
?unnest_tokens
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, ouput = tokens, input = affordance_response)
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, ouput = token, input = affordance_response)
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, input = affordance_response)
##Spelling
#Extract a list of words
tokens = unnest_tokens(tbl = dat, output = token, input = affordance_response)
wordlist = unique(tokens$token)
View(dat)
#spell check
spelling.errors = hunspell(wordlist)
spelling.errors = unique(unlist(spelling.errors))
spellilng.sugg = hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
#Pick the first spelling suggestion
spelling.sugg = unlist(lapply(spelling.sugg, function(x) x[1]))
spelling.sugg = hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
#Pick the first spelling suggestion
spelling.sugg = unlist(lapply(spelling.sugg, function(x) x[1]))
#manually check errors
spell_check = cbind(spelling.sugg, spelling.errors)
View(spell_check)
#Write to file and manually confirm
write.csv(spell_check, row.names = F)
#Write to file and manually confirm
write.csv(spell_check, file = "spell_check.csv", row.names = F)
#read back in the checked output
spell_check = read.csv("spell_check.csv", stringsAsFactors = F)
spellilng.sugg = spell_check$spelling.sugg
spellilng.sugg = as.list(spell_check$spelling.sugg)
spelling.sugg = as.list(spell_check$spelling.sugg)
#Now make a spelling dictionary
spelling.sugg = tolower(spelling.sugg)
#Now make a spelling dictionary
spelling.sugg = tolower(as.list(spelling.sugg))
spelling.sugg = as.list(spelling.sugg)
spelling.dict = as.data.frame(cbind(spelling.errors, spelling.sugg))
spelling.dict$spelling.attern = paste0("\\b", splling.dict$spelling.erros,"\\b")
spelling.dict$spelling.attern = paste0("\\b", spelling.dict$spelling.erros,"\\b")
View(spelling.dict)
spelling.dict$spelling.pattern <-paste0("\\b", spelling.dict$spelling.errors, "\\b")
#write spelling dictionary to .csv
write.csv(x = spelling.dict, file = "../output_data/spelling.dict.csv",
fileEncoding = "utf8", row.names = F)
#write spelling dictionary to .csv
write.csv(x = spelling.dict, file = "spelling.dict.csv",
fileEncoding = "utf8", row.names = F)
#write spelling dictionary to .csv
write.csv(x = spelling.dict, file = "spelling.dict.csv", row.names = F)
#write spelling dictionary to .csv
write.csv(spelling.dict, file = "spelling.dict.csv", row.names = F)
#write spelling dictionary to .csv
write.csv(spelling.dict, file = "spelling.dict.csv", row.names = F)
