#Spelling
library(hunspell)
library(tidytext)
library(stringi)
library(stringr)
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
library(textstem)
library(udpipe)
#stopwords
library(stopwords)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don’t"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he’s"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it’s"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
##okay, I think this does what I want.
no_stop_final$affordance_lemma = lemmatize_strings(no_stop_final$final_affordance)
#fix weird lemmas
no_stop_final$affordance_lemma[no_stop_final$affordance_lemma == "spin-dry"] = "dry"
##Write to file
#write.csv(no_stop_final, file = "lemmatized.csv", row.names = F) #Spot check and clean in Excel
View(no_stop)
View(no_stop_final)
#try another package
no_stop_final$affordance_lemma2 = lemmatize_words(no_stop_final$final_affordance)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
View(dat)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
View(no_stop)
View(no_stop)
##compound words
no_stop$cleaned[no_stop$cleaned == "ice cream"] = "icecream"
View(no_stop)
##compound words
no_stop$cleaned[no_stop$cleaned == "ice cream"] = "icecream"
no_stop$cleaned[no_stop$cleaned == "make up"] = "makeup"
no_stop$cleaned[no_stop$cleaned == "thrown away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throw away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throwing away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "hi school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high five"] = "highfive"
no_stop$cleaned[no_stop$cleaned == "hi five"] = "highfive"
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don’t"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he’s"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it’s"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
View(no_stop_final)
no_stop_final$parsed[no_stop_final$parsed == "n/a"] = NA
no_stop_final$parsed[no_stop_final$parsed == "hail/call"] = NA
no_stop_final$parsed[no_stop_final$parsed == "fish/insect food"] = NA
no_stop_final$parsed[no_stop_final$parsed == "cover/smother"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "buyingselling"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "boatship"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "drawingwriting"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "foodsdesserts"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "onoff"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "andor"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "presentgift"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "ac"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "skincaremakeup"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "picturespicture"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
#let's try udpipe one more time
udpipe(no_stop_final$parsed, "english")
#let's try udpipe one more time
ud_out = udpipe(no_stop_final$parsed, "english")
View(ud_out)
View(no_stop_final)
#figure out how to add cue
no_stop_final$key = rep(1:nrow(no_stop_final))
ud_out$key = substring(ud_out$doc_id, 3)
ud_out$key = substring(ud_out$doc_id, 4)
no_stop_key = no_stop_final[ , c(4, 10)]
merged = merge(ud_out, no_stop_key, "key")
View(merged)
View(no_stop_key)
##figure out what to drop
table(merged$upos)
subset(merged,
merged$upos != "PUNCT")
merged = subset(merged,
merged$upos != "PUNCT")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$upos != "PART")
##remove cans, woulds, shoulds, coulds,
merged = subset(merged,
merged$lemma !="can")
merged = subset(merged,
merged$lemma !="would")
merged = subset(merged,
merged$lemma !="could")
merged = subset(merged,
merged$lemma !="have")
merged = subset(merged,
merged$lemma !="should")
merged = subset(merged,
merged$lemma !="a")
merged = subset(merged,
merged$lemma !="an")
merged = subset(merged,
merged$lemma !="of")
merged = subset(merged,
merged$lemma !="had")
merged = subset(merged,
merged$lemma !="the")
merged = subset(merged,
merged$lemma !="who")
merged = subset(merged,
merged$lemma !="what")
merged = subset(merged,
merged$lemma !="where")
merged = subset(merged,
merged$lemma !="why")
merged = subset(merged,
merged$lemma !="'s")
merged = subset(merged,
merged$lemma !="be")
merged = subset(merged,
merged$lemma !="do")
merged = subset(merged,
merged$lemma !="get")
merged = subset(merged,
merged$lemma !="may")
merged = subset(merged,
merged$lemma !="maybe")
merged = subset(merged,
merged$lemma !="roor")
merged = subset(merged,
merged$lemma !="will")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$upos != "PUNCT")
merged = subset(merged,
merged$upos != "NUM")
merged = subset(merged,
merged$upos != "PRON")
merged = subset(merged,
merged$upos != "SCONJ")
merged = subset(merged,
merged$upos != "ADV")
merged = subset(merged,
merged$lemma !="dino")
merged$lemma !="ok)
merged = subset(merged,
merged$lemma !="ok")
merged = subset(merged,
merged$lemma !="nono")
merged = subset(merged,
merged$lemma !="no")
merged = subset(merged,
merged$lemma !="okay")
merged = subset(merged,
merged$lemma !="yes")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$lemma !="any")
merged = subset(merged,
merged$lemma !="every")
merged = subset(merged,
merged$lemma !="all")
merged = subset(merged,
merged$lemma !="this")
merged = subset(merged,
merged$lemma !="some")
merged = subset(merged,
merged$lemma !="that")
merged = subset(merged,
merged$lemma !="those")
merged = subset(merged,
merged$lemma !="quite")
merged = subset(merged,
merged$lemma !="each")
merged = subset(merged,
merged$lemma !="oto")
merged = subset(merged,
merged$lemma !="other")
merged = subset(merged,
merged$lemma !="another")
merged = subset(merged,
merged$lemma !="both")
#keep it going
table(merged$upos)
merged = subset(merged,
merged$upos != "ADP")
merged = subset(merged,
merged$lemma !="sandwhic")
#keep it going
table(merged$Xpos)
#keep it going
table(merged$xpos)
merged = subset(merged,
merged$lemma !="anti")
merged = subset(merged,
merged$lemma !="s")
merged = subset(merged,
merged$lemma !="semi")
merged = subset(merged,
merged$lemma !="non")
#keep it going
table(merged$xpos)
merged = subset(merged,
merged$lemma !="and")
merged = subset(merged,
merged$lemma !="or")
merged = subset(merged,
merged$lemma !="but")
merged$lemma[merged$lemma == "headbut"] = "headbutt"
#keep it going
table(merged$upos)
merged = subset(merged,
merged$lemma !="stuff")
merged = subset(merged,
merged$lemma !="thing")
merged = subset(merged,
merged$lemma !="lol")
merged = subset(merged,
merged$lemma !=" ")
merged = subset(merged,
merged$lemma !="omg")
merged = subset(merged,
merged$lemma !="ha")
merged = subset(merged,
merged$lemma !="n")
merged = subset(merged,
merged$lemma !="x")
##take a look in excel
write.csv(merged, file = "udpipe_3_18.csv", row.names = F)
####Remove duplicated rows####
dat = read.csv("udpipe_3_18.csv")
distinct(dat)
dat2 = distinct(dat)
View(dat)
dat2 = distinct(dat, sentence)
dat2 = distinct(dat, c(sentence, lemmma))
unique(dat[c("sentence", "lemma")])
dat2 = unique(dat[c("sentence", "lemma")])
View(dat2)
df %>% distinct(sentence, lemma)
dat %>% distinct(sentence, lemma)
dat2 = dat %>% distinct(sentence, lemma)
dat2 = dat %>% distinct(sentence, lemma, .keep_all = T)
View(dat2)
table(dat2$lemma)
##write to .csv for inspection
write.csv(dat2, file = "no_duplicates_3_19.csv")
##write to .csv for inspection
write.csv(dat2, file = "no_duplicates_3_19.csv", row.names = F)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/BOI-Norms/4 Analyses/R/All Cleaned Output/UCONN/Batch 1 August 22 - December 22")
##This code is adapted from Buchanan et al. 2019's primer on processing feature production norms
#https://link.springer.com/article/10.1007/s10339-019-00939-6
####Set up####
##libraries
#Not sure if I'll be using all of these
library(dplyr)
library(here)
#Spelling
library(hunspell)
library(tidytext)
library(stringi)
library(stringr)
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
library(textstem)
library(udpipe)
#stopwords
library(stopwords)
##read in data
master = read.csv("0-Data/Merged_UCONN_12_8.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
source("Scripts/remove idk.R")
#remove other weirdness
dat$affordance_response[dat$affordance_response == "not sure what that is"] = NA
dat$affordance_response[dat$affordance_response == "dont know what this is"] = NA
dat$affordance_response[dat$affordance_response == "dont know what thisis"] = NA
dat$affordance_response[dat$affordance_response == "i'm not sure what a ledger is used for"] = NA
dat$affordance_response[dat$affordance_response == "no idwa"] = NA
dat$affordance_response[dat$affordance_response == "not an object"] = NA
#Check for NAs
table(is.na(dat$affordance_response))
#remove nas
dat = na.omit(dat)
####Fix Spelling and Remove White Space####
##Spelling
#Extract a list of words
#tokens = unnest_tokens(tbl = dat, output = token, input = affordance_response)
parsed_afforances = unnest_tokens(tbl = dat, output = parsed,
input = affordance_response, token = "regex",
pattern = ", ")
parsed_afforances = unnest_tokens(tbl = parsed_afforances, output = parsed,
input = parsed, token = "regex",
pattern = ",")
wordlist = unique(parsed_afforances$parsed)
#Run the spell check
spelling.errors = hunspell(wordlist)
spelling.errors = unique(unlist(spelling.errors))
spelling.sugg = hunspell_suggest(spelling.errors, dict = dictionary("en_US"))
#Pick the first spelling suggestion
spelling.sugg = unlist(lapply(spelling.sugg, function(x) x[1]))
#manually check errors
spell_check = cbind(spelling.sugg, spelling.errors)
#Write to file and manually confirm
#write.csv(spell_check, file = "spell_check_raw.csv", row.names = F)
#read back in the checked output
spell_check = read.csv("spell_check.csv", stringsAsFactors = F)
spelling.sugg = as.list(spell_check$spelling.sugg)
#get the correct number of observations and make a spelling dictionary
spelling.errors = as.data.frame(spelling.errors)
spelling.dict = (merge(spelling.errors, spell_check, by = 'spelling.errors'))
spelling.dict$spelling.sugg = tolower(spelling.dict$spelling.sugg)
spelling.dict$spelling.pattern = paste0("\\b", spelling.dict$spelling.errors, "\\b")
##Remove white spaces and replace misspellings
parsed_afforances = parsed_afforances[!parsed_afforances$parsed =="", ]
parsed_afforances = unnest_tokens(tbl = parsed_afforances, output = parsed,
input = parsed, token = "regex",
pattern = ",")
parsed_afforances$corrected = stri_replace_all_regex(str = parsed_afforances$parsed,
pattern = spelling.dict$spelling.pattern,
replacement = spelling.dict$spelling.sugg,
vectorize_all = FALSE)
##Remove a few weird things
#parsed_afforances$corrected[parsed_afforances$corrected == "na"] = NA
#parsed_afforances$corrected[parsed_afforances$corrected == " "] = NA
#parsed_afforances$corrected[parsed_afforances$corrected == ""] = NA
#parsed_afforances$corrected[parsed_afforances$corrected == "  learning how to do a task"] = "someone learning how to do a task"
#Fix column names
colnames(parsed_afforances)[6:7] = c("affordance_parsed", "affordance_corrected")
##Write spelled checked data to .csv
write.csv(parsed_afforances, file = "spell_checked.csv", row.names = F)
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
##Erin's walkthrough used TreeTagger for this
##Okay, TreeTagger HATES windows and I can't get it to work, what else can I do?
##Treetagger also pulls part of speech, which would be nice to have.
##what about udpipe?
#Example
#x = c(doc_a = "In our last meeting, someone said that we are meeting again tomorrow",
#      doc_b = "It's better to be good at being the best")
#anno = udpipe(x, "english")
#anno[, c("doc_id", "sentence_id", "token", "lemma", "upos")]
##Oooh, I like this as an alternative to treetagger.
#does it work on DF columns?
#lemmatized = udpipe(dat$affordance_corrected, "english")
#It does, but there are a few issues. Mainly it ended up with a few extra tokens somehow.
#Get only the overlapping tokens.
#Okay, I think i figured this out. If any words still have spaces in them, udpipe breaks it in two.
#I thought I had removed spaces up above but something must have gotten off.
#Okay, looks like some spaces get reintroduced line 82 when fixing spelling (e.g., "peprally" got turned back into "pep rally")
#remove spaces from the middle of words
dat$affordance_corrected = stringr::str_remove_all(dat$affordance_corrected, " ")
#Stop words also mess it up. Remove stop words here:
#Now I need to remove punctuation/weirdness
##Remove stop words
no_stop = tokens %>%
filter(!grepl("[[:punct:]]", word)) %>% #Remove punctuation
filter(!word %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", word)) %>% #remove numbers
filter(!is.na(word))
temp = data.frame(table(no_stop$word))
#remove extra spaces
#no_stop$affordance_corrected = gsub(" ", "", no_stop$affordance_corrected)
#Remove 's
#no_stop$affordance_corrected = gsub("'s", "", no_stop$affordance_corrected)
#Remove 't
#no_stop$affordance_corrected = gsub("'t", "t", no_stop$affordance_corrected)
#Remove any stopwords that might have been generated in lines 139/142
#no_stop = no_stop %>%
# filter(!affordance_corrected %in% stopwords(language = "en", source = "snowball"))
##Write to .csv for lemmatization w/ Python
#write.csv(no_stop, file = "cleaned_9_19_21.csv", row.names = F)
####Lemmatize w/ R####
##Having some issues w/ this, using Python instead. Code is included though in case I ever get this working.
#Lemmatize! (This gives a second set of lemmas using a different algorithm. Also provides part of speech info)
lemmatized = udpipe(no_stop$word, "english")
#If lemmatized and no_stop don't match up perfectly, can use the code below to see where the differences are
#lemmatized$token[!lemmatized$token %in% no_stop$affordance_corrected] #Then just tweak the character removal process above as needed
##Combine datasets and add in second set of Lemmas, part of speech info
temp = no_stop$word
temp2 = lemmatized$sentence
temp3 = as.data.frame(temp == temp2[-9400])
combined = cbind(no_stop, lemmatized[-6755 , c(10:11, 13)])
#fix snot
combined[6755, 8] = "snot"
#Give useful column names
colnames(combined)[9] = c("POS")
#Drop unused columns
combined = combined[ , -c(2, 10)]
#don't know why it keeps changing dry to spin-dry though. Going to manually fix that.
combined$lemma[combined$lemma == "spin-dry"] = "dry"
combined$lemma[combined$lemma == "smoothy"] = "smoothie"
#might be a good idea to open this up in excel and spot check
##Write to .csv
#write.csv(combined, file = "UCONN  Cleaned 12_8_22_2.csv", row.names = F)
##Write to .csv
write.csv(combined, file = "UCONN  Cleaned 12_8_22_2.csv", row.names = F)
