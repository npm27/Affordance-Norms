install.packages("textstem")
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don’t"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he’s"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it’s"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
##okay, I think this does what I want.
no_stop_final$affordance_lemma = lemmatize_strings(no_stop_final$final_affordance)
#fix weird lemmas
no_stop_final$affordance_lemma[no_stop_final$affordance_lemma == "spin-dry"] = "dry"
##Write to file
#write.csv(no_stop_final, file = "lemmatized.csv", row.names = F) #Spot check and clean in Excel
##libraries
#Not sure if I'll be using all of these
library(dplyr)
library(here)
#Spelling
library(hunspell)
library(tidytext)
library(stringi)
library(stringr)
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
library(textstem)
library(udpipe)
#stopwords
library(stopwords)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don’t"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he’s"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it’s"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
##okay, I think this does what I want.
no_stop_final$affordance_lemma = lemmatize_strings(no_stop_final$final_affordance)
#fix weird lemmas
no_stop_final$affordance_lemma[no_stop_final$affordance_lemma == "spin-dry"] = "dry"
##Write to file
#write.csv(no_stop_final, file = "lemmatized.csv", row.names = F) #Spot check and clean in Excel
View(no_stop)
View(no_stop_final)
#try another package
no_stop_final$affordance_lemma2 = lemmatize_words(no_stop_final$final_affordance)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
View(dat)
####get formatting consistent####
##read in data
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
##remove a few more idks that got missed
dat$Initial[dat$Initial == "i do not know what this is "] = NA
dat$Initial[dat$Initial == "to be honest i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know how to do it"] = NA
dat$Initial[dat$Initial == "do not know"] = NA
dat$Initial[dat$Initial == "do not know "] = NA
dat$Initial[dat$Initial == "do not know what this is "] = NA
dat$Initial[dat$Initial == "do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what this is"] = NA
dat$Initial[dat$Initial == "i do not know what a physiologist is. "] = NA
dat$Initial[dat$Initial == "this is a person. i hope you would do nothing else but receive the newspaper from this newsboy"] = NA
dat = na.omit(dat)
##Remove any punctuation and white space
dat$cleaned = gsub("[[:punct:]]", "", dat$Initial)
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", cleaned)) %>% #Remove any final punctuation
filter(!cleaned %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", cleaned)) %>% #remove numbers
filter(!is.na(cleaned))
View(no_stop)
View(no_stop)
##compound words
no_stop$cleaned[no_stop$cleaned == "ice cream"] = "icecream"
View(no_stop)
##compound words
no_stop$cleaned[no_stop$cleaned == "ice cream"] = "icecream"
no_stop$cleaned[no_stop$cleaned == "make up"] = "makeup"
no_stop$cleaned[no_stop$cleaned == "thrown away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throw away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "throwing away"] = "throwaway"
no_stop$cleaned[no_stop$cleaned == "hi school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high school"] = "highschool"
no_stop$cleaned[no_stop$cleaned == "high five"] = "highfive"
no_stop$cleaned[no_stop$cleaned == "hi five"] = "highfive"
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
##Now lemmatize
#extract updated tokens
tokens = unnest_tokens(tbl = no_stop, output = final_affordance, input = cleaned)
#now remove stop words one more time
no_stop_final = tokens %>%
filter(!final_affordance %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!is.na(final_affordance ))
#remove a couple of weird responses
no_stop_final$final_affordance[no_stop_final$final_affordance == "aa"] = NA
#remove a few other common words
no_stop_final$final_affordance[no_stop_final$final_affordance == "able"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "can"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "thankyou"] = "thank"
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "youre"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "wouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "shouldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "couldnt"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "don’t"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "he’s"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "it’s"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
View(no_stop_final)
no_stop_final$parsed[no_stop_final$parsed == "n/a"] = NA
no_stop_final$parsed[no_stop_final$parsed == "hail/call"] = NA
no_stop_final$parsed[no_stop_final$parsed == "fish/insect food"] = NA
no_stop_final$parsed[no_stop_final$parsed == "cover/smother"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "buyingselling"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "boatship"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "drawingwriting"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "foodsdesserts"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "onoff"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "andor"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "presentgift"] = NA
no_stop_final$final_affordance[no_stop_final$final_affordance == "ac"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "skincaremakeup"] = NA
no_stop_final = na.omit(no_stop_final)
no_stop_final$final_affordance[no_stop_final$final_affordance == "picturespicture"] = NA
no_stop_final = na.omit(no_stop_final)
#get the cue items
cuelist = unique(no_stop_final$Stimuli.Cue)
#let's try udpipe one more time
udpipe(no_stop_final$parsed, "english")
#let's try udpipe one more time
ud_out = udpipe(no_stop_final$parsed, "english")
View(ud_out)
View(no_stop_final)
#figure out how to add cue
no_stop_final$key = rep(1:nrow(no_stop_final))
ud_out$key = substring(ud_out$doc_id, 3)
ud_out$key = substring(ud_out$doc_id, 4)
no_stop_key = no_stop_final[ , c(4, 10)]
merged = merge(ud_out, no_stop_key, "key")
View(merged)
View(no_stop_key)
##figure out what to drop
table(merged$upos)
subset(merged,
merged$upos != "PUNCT")
merged = subset(merged,
merged$upos != "PUNCT")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$upos != "PART")
##remove cans, woulds, shoulds, coulds,
merged = subset(merged,
merged$lemma !="can")
merged = subset(merged,
merged$lemma !="would")
merged = subset(merged,
merged$lemma !="could")
merged = subset(merged,
merged$lemma !="have")
merged = subset(merged,
merged$lemma !="should")
merged = subset(merged,
merged$lemma !="a")
merged = subset(merged,
merged$lemma !="an")
merged = subset(merged,
merged$lemma !="of")
merged = subset(merged,
merged$lemma !="had")
merged = subset(merged,
merged$lemma !="the")
merged = subset(merged,
merged$lemma !="who")
merged = subset(merged,
merged$lemma !="what")
merged = subset(merged,
merged$lemma !="where")
merged = subset(merged,
merged$lemma !="why")
merged = subset(merged,
merged$lemma !="'s")
merged = subset(merged,
merged$lemma !="be")
merged = subset(merged,
merged$lemma !="do")
merged = subset(merged,
merged$lemma !="get")
merged = subset(merged,
merged$lemma !="may")
merged = subset(merged,
merged$lemma !="maybe")
merged = subset(merged,
merged$lemma !="roor")
merged = subset(merged,
merged$lemma !="will")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$upos != "PUNCT")
merged = subset(merged,
merged$upos != "NUM")
merged = subset(merged,
merged$upos != "PRON")
merged = subset(merged,
merged$upos != "SCONJ")
merged = subset(merged,
merged$upos != "ADV")
merged = subset(merged,
merged$lemma !="dino")
merged$lemma !="ok)
merged = subset(merged,
merged$lemma !="ok")
merged = subset(merged,
merged$lemma !="nono")
merged = subset(merged,
merged$lemma !="no")
merged = subset(merged,
merged$lemma !="okay")
merged = subset(merged,
merged$lemma !="yes")
##figure out what to drop
table(merged$upos)
merged = subset(merged,
merged$lemma !="any")
merged = subset(merged,
merged$lemma !="every")
merged = subset(merged,
merged$lemma !="all")
merged = subset(merged,
merged$lemma !="this")
merged = subset(merged,
merged$lemma !="some")
merged = subset(merged,
merged$lemma !="that")
merged = subset(merged,
merged$lemma !="those")
merged = subset(merged,
merged$lemma !="quite")
merged = subset(merged,
merged$lemma !="each")
merged = subset(merged,
merged$lemma !="oto")
merged = subset(merged,
merged$lemma !="other")
merged = subset(merged,
merged$lemma !="another")
merged = subset(merged,
merged$lemma !="both")
#keep it going
table(merged$upos)
merged = subset(merged,
merged$upos != "ADP")
merged = subset(merged,
merged$lemma !="sandwhic")
#keep it going
table(merged$Xpos)
#keep it going
table(merged$xpos)
merged = subset(merged,
merged$lemma !="anti")
merged = subset(merged,
merged$lemma !="s")
merged = subset(merged,
merged$lemma !="semi")
merged = subset(merged,
merged$lemma !="non")
#keep it going
table(merged$xpos)
merged = subset(merged,
merged$lemma !="and")
merged = subset(merged,
merged$lemma !="or")
merged = subset(merged,
merged$lemma !="but")
merged$lemma[merged$lemma == "headbut"] = "headbutt"
#keep it going
table(merged$upos)
merged = subset(merged,
merged$lemma !="stuff")
merged = subset(merged,
merged$lemma !="thing")
merged = subset(merged,
merged$lemma !="lol")
merged = subset(merged,
merged$lemma !=" ")
merged = subset(merged,
merged$lemma !="omg")
merged = subset(merged,
merged$lemma !="ha")
merged = subset(merged,
merged$lemma !="n")
merged = subset(merged,
merged$lemma !="x")
##take a look in excel
write.csv(merged, file = "udpipe_3_18.csv", row.names = F)
####Remove duplicated rows####
dat = read.csv("udpipe_3_18.csv")
distinct(dat)
dat2 = distinct(dat)
View(dat)
dat2 = distinct(dat, sentence)
dat2 = distinct(dat, c(sentence, lemmma))
unique(dat[c("sentence", "lemma")])
dat2 = unique(dat[c("sentence", "lemma")])
View(dat2)
df %>% distinct(sentence, lemma)
dat %>% distinct(sentence, lemma)
dat2 = dat %>% distinct(sentence, lemma)
dat2 = dat %>% distinct(sentence, lemma, .keep_all = T)
View(dat2)
table(dat2$lemma)
##write to .csv for inspection
write.csv(dat2, file = "no_duplicates_3_19.csv")
##write to .csv for inspection
write.csv(dat2, file = "no_duplicates_3_19.csv", row.names = F)
setwd("C:/Users/nickm/OneDrive/Documents/GitHub/BOI-Norms/4 Analyses/R/All Cleaned Output/USM/Batch 1 October 20 - March 22")
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
#Not sure if I'll be using all of these
library(dplyr)
library(here)
#Spelling
library(hunspell)
library(tidytext)
library(stringi)
library(stringr)
#Lemmas
library(koRpus)
library(koRpus.lang.en)
library(tokenizers)
library(textstem)
library(udpipe)
#stopwords
library(stopwords)
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
#remove spaces from the middle of words
dat$affordance_corrected = stringr::str_remove_all(dat$affordance_corrected, " ")
##Remove stop words
no_stop = dat %>%
filter(!grepl("[[:punct:]]", affordance_corrected)) %>% #Remove punctuation
filter(!affordance_corrected %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", affordance_corrected)) %>% #remove numbers
filter(!is.na(affordance_corrected))
temp = data.frame(table(no_stop$affordance_corrected)) #If you View temp, you can see that there's parenthesis, puncation, weird spaces, etc.
#remove extra spaces
no_stop$affordance_corrected = gsub(" ", "", no_stop$affordance_corrected)
#Remove 's
no_stop$affordance_corrected = gsub("'s", "", no_stop$affordance_corrected)
#Remove 't
no_stop$affordance_corrected = gsub("'t", "t", no_stop$affordance_corrected)
#Remove any stopwords that might have been generated in lines 139/142
no_stop = no_stop %>%
filter(!affordance_corrected %in% stopwords(language = "en", source = "snowball"))
####Lemmatize w/ R####
##Having some issues w/ this, using Python instead. Code is included though in case I ever get this working.
#Lemmatize! (This gives a second set of lemmas using a different algorithm. Also provides part of speech info)
lemmatized = udpipe(no_stop$affordance_corrected, "english")
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
#remove spaces from the middle of words
dat$affordance_corrected = stringr::str_remove_all(dat$affordance_corrected, " ")
##Remove stop words
no_stop = tokens %>%
filter(!grepl("[[:punct:]]", word)) %>% #Remove punctuation
filter(!word %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", word)) %>% #remove numbers
filter(!is.na(word))
temp = data.frame(table(no_stop$word))
####Lemmatize w/ R####
##Having some issues w/ this, using Python instead. Code is included though in case I ever get this working.
#Lemmatize! (This gives a second set of lemmas using a different algorithm. Also provides part of speech info)
lemmatized = udpipe(no_stop$word, "english")
####Lemmatization####
dat = read.csv("spell_checked.csv", stringsAsFactors = F)
#extract updated tokens
tokens = unnest_tokens(tbl = dat, output = word, input = affordance_corrected)
cuelist = unique(tokens$Stimuli.Cue)
##okay, I think this does what I want.
dat$affordance_lemma = lemmatize_strings(dat$affordance_corrected)
#remove spaces from the middle of words
dat$affordance_corrected = stringr::str_remove_all(dat$affordance_corrected, " ")
##Remove stop words
no_stop = tokens %>%
filter(!grepl("[[:punct:]]", word)) %>% #Remove punctuation
filter(!word %in% stopwords(language = "en", source = "snowball")) %>% #remove stopwords
filter(!grepl("[[:digit:]]+", word)) %>% #remove numbers
filter(!is.na(word))
temp = data.frame(table(no_stop$word))
####Lemmatize w/ R####
##Having some issues w/ this, using Python instead. Code is included though in case I ever get this working.
#Lemmatize! (This gives a second set of lemmas using a different algorithm. Also provides part of speech info)
lemmatized = udpipe(no_stop$word, "english")
##read in data
master = read.csv("0-Data/combined_data_3_16_22.csv", stringsAsFactors = F)
##only keep the columns we need
dat = master[ , c(1, 5, 11, 13, 19, 33, 35)]
#useful column names
colnames(dat)[6] = "affordance_response"
#make blank cells NA
dat$affordance_response[dat$affordance_response == ""] = NA
dat$affordance_response[dat$affordance_response == "n/a"] = NA
##normalize all responses to lowercase
dat$affordance_response = tolower(dat$affordance_response)
